{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAC coloring search\n",
    "\n",
    "In this notebook we provide utils to run benchmarks and experiment with our code.\n",
    "\n",
    "In the first section we start with utility functions, in the second part we load/generate benchmark data. After we run individual benchmarks on selected graph classes with selected algorithms. The algorithms are described in that section.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir benchmarks/logs/nac\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import importlib\n",
    "from random import Random\n",
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline as backend_inline\n",
    "from matplotlib.backends import backend_agg\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import signal\n",
    "import itertools\n",
    "import base64\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nac as nac\n",
    "import nac.util\n",
    "from benchmarks import dataset\n",
    "importlib.reload(nac)\n",
    "importlib.reload(nac.util)\n",
    "importlib.reload(dataset)\n",
    "\n",
    "seed=42\n",
    "TEST=False\n",
    "\n",
    "OUTPUT_DIR = os.path.join(\"benchmarks\", \"runs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/75898999\n",
    "from typing import Callable, TypeVar, ParamSpec\n",
    "\n",
    "P = ParamSpec(\"P\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "def copy_doc(wrapper: Callable[P, T]):\n",
    "    \"\"\"An implementation of functools.wraps.\"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable[P, T]:\n",
    "        func.__doc__ = wrapper.__doc__\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@copy_doc(plt.figure)\n",
    "def figure(num: Any = 1, *args, **kwargs) -> Figure:\n",
    "    \"\"\"Creates a figure that is independent on the global plt state\"\"\"\n",
    "    fig = Figure(*args, **kwargs)\n",
    "    def show():\n",
    "        manager = backend_agg.new_figure_manager_given_figure(num, fig)\n",
    "        display(\n",
    "            manager.canvas.figure,\n",
    "            metadata=backend_inline._fetch_figure_metadata(manager.canvas.figure),\n",
    "        )\n",
    "        manager.destroy()\n",
    "    fig.show = show\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading locally stored graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphs:\n",
    "    laman = list(dataset.load_laman_graphs())\n",
    "    laman_random = list(dataset.load_laman_random_graphs())\n",
    "    laman_deg_3_plus = list(dataset.load_laman_degree_3_plus())\n",
    "    no_3_nor_4_cycles = dataset.load_no_3_nor_4_cycle_graphs()\n",
    "    sparse_graphs = dataset.generate_sparse_graphs(30, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell bellow generates random laman graphs and stores them as `./benchmarks/graph-store/laman-random/laman_{n}.g6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~1h 30m on my laptop\n",
    "def generate_random_laman_graphs(\n",
    "    LAMAN_DIR: str = os.path.join(\"benchmarks\", \"graphs-store\", \"laman-random\"),\n",
    ") -> List[Tuple[int, List[nx.Graph]]]:\n",
    "    os.makedirs(LAMAN_DIR, exist_ok=True)\n",
    "\n",
    "    ranges = (\n",
    "        (10, 20, 128),\n",
    "        (20, 30, 64),\n",
    "        (30, 40, 32),\n",
    "        (40, 50, 16),\n",
    "        (50, 60, 8),\n",
    "    )\n",
    "    configs = [(n, c) for l, h, c in ranges for n in range(l, h)]\n",
    "    results: List[Tuple[int, List[nx.Graph]]] = []\n",
    "    for n, count in tqdm(configs):\n",
    "        graphs = dataset.generate_laman_graphs(\n",
    "            nodes_l=n,\n",
    "            nodes_h=n,\n",
    "            count=count,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        path = os.path.join(LAMAN_DIR, f\"laman_{n}.g6\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            for graph in graphs:\n",
    "                f.write(nx.graph6.to_graph6_bytes(graph, header=False))\n",
    "\n",
    "        results.append((n, graphs))\n",
    "    return results\n",
    "\n",
    "if False:\n",
    "    random_laman_graphs = generate_random_laman_graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running benchmarks\n",
    "\n",
    "Target columns are\n",
    "- `graph` - graph6 encoded graph\n",
    "- `dataset` - class of the graph, `laman`, `laman_deg_3_plus`, `no_3_nor_4_cycles`, `sparse`\n",
    "- `mode` - search mode: [`single`, `all`]\n",
    "- `vertex_no` - the number of vertices of the graph\n",
    "- `edge_no` - the number of edges of the graph\n",
    "- `triangle_components_no` - the number of triangle components of the graph\n",
    "- `monochromatic_classes_no` - the number of triangle components of the graph\n",
    "- `relabel` - relabel strategy\n",
    "- `split` - splitting strategy\n",
    "- `merging` - merging strategy\n",
    "- `subgraph_size` - the initial size of subgraphs in components\n",
    "- `nac_coloring_no` - the number of NAC colorings of the graph\n",
    "- `nac_mean_time` - the time required to find all the colorings with the given strategy in miliseconds\n",
    "- `nac_rounds` - number of rounds used to run the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS: List[str] = [\n",
    "    \"graph\",\n",
    "    \"dataset\",\n",
    "    \"vertex_no\",\n",
    "    \"edge_no\",\n",
    "    \"triangle_components_no\",\n",
    "    \"monochromatic_classes_no\",\n",
    "    \"relabel\",\n",
    "    \"split\",\n",
    "    \"merging\",\n",
    "    \"subgraph_size\",\n",
    "    \"nac_any_finished\",\n",
    "    \"nac_first_coloring_no\",\n",
    "    \"nac_first_mean_time\",\n",
    "    \"nac_first_rounds\",\n",
    "    \"nac_first_checks\",\n",
    "    \"nac_all_coloring_no\",\n",
    "    \"nac_all_mean_time\",\n",
    "    \"nac_all_rounds\",\n",
    "    \"nac_all_checks\",\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class MeasurementResult:\n",
    "    graph: str\n",
    "    dataset: str\n",
    "    vertex_no: int\n",
    "    edge_no: int\n",
    "    triangle_components_no: int\n",
    "    monochromatic_classes_no: int\n",
    "    relabel: str\n",
    "    split: str\n",
    "    merging: str\n",
    "    subgraph_size: int\n",
    "    nac_any_finished: bool\n",
    "    nac_first_coloring_no: Optional[int]\n",
    "    nac_first_mean_time: Optional[int]\n",
    "    nac_first_rounds: Optional[int]\n",
    "    nac_first_checks: Optional[int]\n",
    "    nac_all_coloring_no: Optional[int]\n",
    "    nac_all_mean_time: Optional[int]\n",
    "    nac_all_rounds: Optional[int]\n",
    "    nac_all_checks: Optional[int]\n",
    "\n",
    "    def to_list(self) -> List:\n",
    "        return [\n",
    "            self.graph,\n",
    "            self.dataset,\n",
    "            self.vertex_no,\n",
    "            self.edge_no,\n",
    "            self.triangle_components_no,\n",
    "            self.monochromatic_classes_no,\n",
    "            self.relabel,\n",
    "            self.split,\n",
    "            self.merging,\n",
    "            self.subgraph_size,\n",
    "            self.nac_any_finished,\n",
    "            self.nac_first_coloring_no,\n",
    "            self.nac_first_mean_time,\n",
    "            self.nac_first_rounds,\n",
    "            self.nac_first_checks,\n",
    "            self.nac_all_coloring_no,\n",
    "            self.nac_all_mean_time,\n",
    "            self.nac_all_rounds,\n",
    "            self.nac_all_checks,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Promissing:\n",
    "    RELABELING = [\n",
    "        \"none\",\n",
    "        \"random\",\n",
    "        # \"bfs\",\n",
    "    ]\n",
    "    SPLITTING = [\n",
    "        \"none\",\n",
    "        \"neighbors\",\n",
    "        \"neighbors_degree\",\n",
    "    ]\n",
    "    MERGING_OFFLINE = [\n",
    "        \"linear\",\n",
    "        \"log\",\n",
    "        \"score\",\n",
    "        \"shared_vertices\"\n",
    "    ]\n",
    "    MERGING_ONLINE = [\n",
    "        \"linear\",\n",
    "        \"log\",\n",
    "        \"shared_vertices\"\n",
    "    ]\n",
    "    SIZES = [6]\n",
    "\n",
    "    strategies_offline = list(itertools.product(\n",
    "        RELABELING, SPLITTING, MERGING_OFFLINE, [4], #SIZES,\n",
    "    ))\n",
    "    strategies_online = list(itertools.product(\n",
    "        RELABELING, SPLITTING, MERGING_ONLINE, [6], #SIZES,\n",
    "    ))\n",
    "print(f\"Offline strategies: {len(Promissing.strategies_offline)}\")\n",
    "print(f\"Online strategies:  {len(Promissing.strategies_online)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_id(graph: nx.Graph) -> str:\n",
    "    return base64.standard_b64encode(nx.graph6.to_graph6_bytes(graph, header=False).strip()).decode()\n",
    "\n",
    "def graph_from_id(id: str) -> nx.Graph:\n",
    "    return nac.util.NiceGraph(nx.graph6.from_graph6_bytes(base64.standard_b64decode(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_DataFrame(data: List[MeasurementResult] = []) -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        [x.to_list() for x in data],\n",
    "        columns=COLUMNS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BENCH_FILE_START = \"bench_res_v1\"\n",
    "def load_records(file_name: str | None = None, dir = OUTPUT_DIR) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the results from the last run or the run specified by `file_name` in the `dir` given.\n",
    "    \"\"\"\n",
    "    if file_name == None:\n",
    "        def filter_cond(name: str) -> bool:\n",
    "            return name.startswith(_BENCH_FILE_START) and name.endswith(\".csv\")\n",
    "        data = sorted(filter(filter_cond, os.listdir(dir)), reverse=True)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            print(\"No file with results found!\")\n",
    "            return new_DataFrame()\n",
    "        file_name = data[0]\n",
    "        print(f\"Found file: {file_name}\")\n",
    "\n",
    "    path = os.path.join(dir, file_name)\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def store_results(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str | None = None,\n",
    "    dir = OUTPUT_DIR,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Stores results in the given file\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_name = f\"{_BENCH_FILE_START}_{current_time}.csv\"\n",
    "    path = os.path.join(dir, file_name)\n",
    "    df.to_csv(path, header=True, index=False)\n",
    "    return file_name\n",
    "\n",
    "def update_stored_data(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    df = load_records()\n",
    "    display(df)\n",
    "    df = pd.concat((df, pd.concat(dfs)))\n",
    "    df = df.drop_duplicates(\n",
    "        subset=[\"graph\", \"dataset\"],\n",
    "        keep='last',\n",
    "    )\n",
    "    store_results(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strategy(param: Tuple[str, str, str, int]) -> Tuple[str, str]:\n",
    "    relabel, split, merge, subgraph = param\n",
    "    algo_name = \"subgraphs-{}-{}-{}-smart\".format(\n",
    "        merge, split, subgraph\n",
    "    )\n",
    "    return (relabel, algo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkTimeoutException(Exception):\n",
    "    def __init__(self, msg: str = \"The benchmark timed out\", *args, **kwargs):\n",
    "        super().__init__(msg, *args, **kwargs)\n",
    "\n",
    "\n",
    "def with_timeout[**T, R, D](function: Callable[T, R], time_limit: int | None, default: D) -> Callable[T, R|D]:\n",
    "    if time_limit is None:\n",
    "        return function\n",
    "\n",
    "    def impl(*args: P.args, **kwargs: P.kwargs):\n",
    "        try:\n",
    "            # signals are not exact, but generally work\n",
    "            def timeout_handler(signum, frame):\n",
    "                raise BenchmarkTimeoutException()\n",
    "            signal.signal(signal.SIGALRM, timeout_handler)\n",
    "            signal.alarm(time_limit)\n",
    "\n",
    "            res = function(*args, **kwargs)\n",
    "\n",
    "            signal.alarm(0)\n",
    "            return res\n",
    "        except BenchmarkTimeoutException:\n",
    "            return default\n",
    "    return impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MeasuredRecord:\n",
    "    time_sum: int\n",
    "    coloring_no: int\n",
    "    rounds: int\n",
    "    checks_performed: int\n",
    "\n",
    "    @property\n",
    "    def mean_time(self) -> int:\n",
    "        if self.rounds == 0:\n",
    "            return 0\n",
    "        return self.time_sum / self.rounds\n",
    "\n",
    "@dataclass\n",
    "class MeasuredData:\n",
    "    first: Optional[MeasuredRecord]\n",
    "    all: Optional[MeasuredRecord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nac_benchmark_core(\n",
    "    graph: nx.Graph,\n",
    "    rounds: int,\n",
    "    first_only: bool,\n",
    "    strategy: Tuple[str, str],\n",
    "    time_limit: int,\n",
    "    seed: int | None = 42,\n",
    ") -> MeasuredData:\n",
    "    \"\"\"\n",
    "    Runs benchmarks for NAC coloring search\n",
    "    Returns results grouped by relabel, split, merge and subgraph size strategies\n",
    "    \"\"\"\n",
    "\n",
    "    result = MeasuredData(None, None)\n",
    "    rand = random.Random(seed)\n",
    "\n",
    "    def find_colorings():\n",
    "        start_time = time.time()\n",
    "\n",
    "        itr = iter(\n",
    "            nac.NAC_colorings(\n",
    "                graph=graph,\n",
    "                algorithm=strategy[1],\n",
    "                relabel_strategy=strategy[0],\n",
    "                seed=rand.randint(0, 2**30),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        first_col = next(itr, None)\n",
    "        first_time = time.time()\n",
    "\n",
    "        if result.first is None:\n",
    "            result.first = MeasuredRecord(0, 0, 0, 0)\n",
    "        result.first = MeasuredRecord(\n",
    "            time_sum=result.first.time_sum + first_time - start_time,\n",
    "            coloring_no=0 if first_col is None else 1,\n",
    "            rounds=result.first.rounds + 1,\n",
    "            checks_performed=nac.NAC_check_called()[1],\n",
    "        )\n",
    "\n",
    "        if first_only:\n",
    "            return\n",
    "\n",
    "        j = 0\n",
    "        for j, coloring in enumerate(itr): pass\n",
    "        end_time = time.time()\n",
    "\n",
    "        if result.all is None:\n",
    "            result.all = MeasuredRecord(0, 0, 0, 0)\n",
    "        result.all = MeasuredRecord(\n",
    "            time_sum=result.all.time_sum + end_time - start_time,\n",
    "            coloring_no=j+1,\n",
    "            rounds=result.all.rounds + 1,\n",
    "            checks_performed=nac.NAC_check_called()[1],\n",
    "        )\n",
    "\n",
    "    def run() -> None:\n",
    "        [find_colorings() for _ in range(rounds)]\n",
    "\n",
    "    with_timeout(\n",
    "        run,\n",
    "        time_limit=time_limit*rounds,\n",
    "        default=None,\n",
    "    )()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_measurement_result(\n",
    "    graph: nx.Graph,\n",
    "    dataset_name: str,\n",
    "    trianlge_classes: int,\n",
    "    monochromatic_classes: int,\n",
    "    nac_first: Optional[MeasuredRecord],\n",
    "    nac_all: Optional[MeasuredRecord],\n",
    "    relabel_strategy: str,\n",
    "    split_strategy: str,\n",
    "    merge_strategy: str,\n",
    "    subgraph_size: int,\n",
    ") -> MeasurementResult:\n",
    "    vertex_no = nx.number_of_nodes(graph)\n",
    "    edge_no = nx.number_of_edges(graph)\n",
    "\n",
    "    nac_first_coloring_no=0\n",
    "    nac_first_mean_time=0\n",
    "    nac_first_rounds=0\n",
    "    nac_first_checks=0\n",
    "    nac_all_coloring_no=0\n",
    "    nac_all_mean_time=0\n",
    "    nac_all_rounds=0\n",
    "    nac_all_checks=0\n",
    "    nac_any_finished = (nac_first or nac_all) is not None\n",
    "\n",
    "    if nac_first is not None:\n",
    "        nac_first_coloring_no=nac_first.coloring_no\n",
    "        nac_first_mean_time=int(nac_first.mean_time*1000)\n",
    "        nac_first_rounds=nac_first.rounds\n",
    "        nac_first_checks=nac_first.checks_performed\n",
    "    if nac_all is not None:\n",
    "        nac_all_coloring_no=nac_all.coloring_no\n",
    "        nac_all_mean_time=int(nac_all.mean_time*1000)\n",
    "        nac_all_rounds=nac_all.rounds\n",
    "        nac_all_checks=nac_all.checks_performed\n",
    "\n",
    "    return MeasurementResult(\n",
    "        graph=graph_id(graph),\n",
    "        dataset=dataset_name,\n",
    "        vertex_no=vertex_no,\n",
    "        edge_no=edge_no,\n",
    "        triangle_components_no=trianlge_classes,\n",
    "        monochromatic_classes_no=monochromatic_classes,\n",
    "        relabel=relabel_strategy,\n",
    "        split=split_strategy,\n",
    "        merging=merge_strategy,\n",
    "        subgraph_size=subgraph_size,\n",
    "        nac_any_finished=nac_any_finished,\n",
    "        nac_first_coloring_no=nac_first_coloring_no,\n",
    "        nac_first_mean_time=nac_first_mean_time,\n",
    "        nac_first_rounds=nac_first_rounds,\n",
    "        nac_first_checks=nac_first_checks,\n",
    "        nac_all_coloring_no=nac_all_coloring_no,\n",
    "        nac_all_mean_time=nac_all_mean_time,\n",
    "        nac_all_rounds=nac_all_rounds,\n",
    "        nac_all_checks=nac_all_checks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def measure_for_class(\n",
    "    dataset_name: str,\n",
    "    graphs: List[nx.Graph],\n",
    "    all_max_vertex_no: int,\n",
    "    rounds:int,\n",
    "    graph_timeout: int,\n",
    ") -> pd.DataFrame:\n",
    "    dataset_name = dataset_name.replace(\" \", \"_\").lower()\n",
    "    results: List[MeasurementResult] = []\n",
    "    for graph in tqdm(graphs):\n",
    "        all_colorings = all_max_vertex_no >= graph.number_of_nodes()\n",
    "        trianlge_classes = len(nac.find_triangle_components(graph=graph, use_triangles_over_component=False)[1])\n",
    "        monochromatic_classes = len(nac.find_triangle_components(graph=graph, use_triangles_over_component=True)[1])\n",
    "\n",
    "        strategies = Promissing.strategies_offline if all_colorings else Promissing.strategies_online\n",
    "\n",
    "        for strategy in strategies:\n",
    "            try:\n",
    "                search_res = nac_benchmark_core(\n",
    "                    graph,\n",
    "                    rounds=rounds,\n",
    "                    first_only=not all_colorings,\n",
    "                    strategy=create_strategy(strategy),\n",
    "                    time_limit=graph_timeout,\n",
    "                )\n",
    "\n",
    "                relabel, split, merge, subgraph_size = strategy\n",
    "                res = create_measurement_result(\n",
    "                    graph=graph,\n",
    "                    dataset_name=dataset_name,\n",
    "                    trianlge_classes=trianlge_classes,\n",
    "                    monochromatic_classes=monochromatic_classes,\n",
    "                    nac_first=search_res.first,\n",
    "                    nac_all=search_res.all,\n",
    "                    relabel_strategy=relabel,\n",
    "                    split_strategy=split,\n",
    "                    merge_strategy=merge,\n",
    "                    subgraph_size=subgraph_size,\n",
    "                )\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    df = new_DataFrame(results)\n",
    "    df = df.sort_values(by=[\"nac_all_mean_time\", \"nac_first_mean_time\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"Fail here, don't run the remaining cells automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_test = measure_for_class(\n",
    "        \"test\",\n",
    "        # [g for g in Graphs.laman_deg_3_plus if g.number_of_nodes() == 8][:8],\n",
    "        [g for g in Graphs.sparse_graphs if g.number_of_nodes() == 13][:8],\n",
    "        all_max_vertex_no=10,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_laman = measure_for_class(\n",
    "        \"Laman\",\n",
    "        Graphs.laman,\n",
    "        all_max_vertex_no=15,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )\n",
    "    update_stored_data([df_laman])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    df_laman_random = measure_for_class(\n",
    "        \"Laman random\",\n",
    "        Graphs.laman_random,\n",
    "        all_max_vertex_no=16,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )\n",
    "    update_stored_data([df_laman_random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_laman_deg_3_plus = measure_for_class(\n",
    "        \"Laman deg 3+\",\n",
    "        Graphs.laman_deg_3_plus,\n",
    "        # All with 36 strtegies, 3 rounds\n",
    "        #  8 - 1s/it\n",
    "        #  9 - 1s/it\n",
    "        # 10 - 2s/it\n",
    "        # 11 - 7s/it\n",
    "        # 12 - 15s/it -> ~20 mon. classes\n",
    "        # First coloring with 27 strategies, 3 rounds\n",
    "        # 15 - 5s/it\n",
    "        # 16 - 5s/it\n",
    "        # 17 - 90s/it\n",
    "        all_max_vertex_no=12,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )\n",
    "    update_stored_data([df_laman_deg_3_plus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    display(pd.Series([g.number_of_nodes() for g in Graphs.no_3_nor_4_cycles]).value_counts())\n",
    "    df_no_3_nor_4_cycles = measure_for_class(\n",
    "        \"No 3 nor 4 cycles\",\n",
    "        Graphs.no_3_nor_4_cycles,\n",
    "        # 24 strategies\n",
    "        # 10 - 5 s/it\n",
    "        # 11 - 10 s/it\n",
    "        # 12 - 28 s/it\n",
    "        # 13 -\n",
    "        all_max_vertex_no=13,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )\n",
    "    update_stored_data([df_no_3_nor_4_cycles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_sparse = measure_for_class(\n",
    "        \"Sparse\",\n",
    "        Graphs.sparse_graphs,\n",
    "        all_max_vertex_no=15,\n",
    "        rounds=3,\n",
    "        graph_timeout=3,\n",
    "    )\n",
    "    update_stored_data([df_sparse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics\n",
    "\n",
    "Base graphs show the time required to find\n",
    "a first/all NAC coloring based on vertex no./monochromatic classes no.\n",
    "First graphs are separated for each class of graphs and\n",
    "in the end for all the classes combined.\n",
    "Graphs are drawn for each strategy cathegory to compare them easily.\n",
    "Graphs show mean, median and 1st quartil values of running times to lower bias.\n",
    "\n",
    "Second group of graphs shows our contribution of decresing\n",
    "the number of `is_NAC_coloring` checks called compared to\n",
    "the naive approach without or with triangle/monochromatic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmarks = load_records()\n",
    "display(df_benchmarks.info())\n",
    "display(list(df_benchmarks[\"dataset\"].unique()))\n",
    "display(list(df_benchmarks[\"relabel\"].unique()))\n",
    "display(list(df_benchmarks[\"split\"].unique()))\n",
    "display(list(df_benchmarks[\"merging\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _group_and_plot(\n",
    "    df: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    x_column: Literal[\"vertex_no\", \"monochromatic_classes_no\"],\n",
    "    based_on: Literal[\"relabel\", \"split\", \"merging\"],\n",
    "    value_column: Literal[\"nac_first_mean_time\", \"nac_all_mean_time\"],\n",
    "    aggregation: Literal[\"mean\", \"median\", \"quartil\"]\n",
    "):\n",
    "    df = df.loc[:, [x_column, based_on, value_column]]\n",
    "    groupped = df.groupby([x_column, based_on])\n",
    "    match aggregation:\n",
    "        case \"mean\":\n",
    "            aggregated = groupped.mean()\n",
    "        case \"median\":\n",
    "            aggregated = groupped.median()\n",
    "        case \"quartil\":\n",
    "            aggregated = groupped.quantile(.25)\n",
    "\n",
    "    aggregated = aggregated.reorder_levels([based_on, x_column], axis=0)\n",
    "\n",
    "    for name in aggregated.index.get_level_values(based_on).unique():\n",
    "        data = aggregated.loc[name]\n",
    "        ax.plot(data.index, data[value_column], label=name)\n",
    "\n",
    "    ax.set_title(f\"{x_column} {based_on} ({aggregation})\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.legend()\n",
    "\n",
    "def plot_frame(\n",
    "    title: str,\n",
    "    df: pd.DataFrame,\n",
    ") -> List[Figure]:\n",
    "    print(f\"Plotting {df.shape[0]} records...\")\n",
    "    figs = []\n",
    "\n",
    "    ops_value_column = [\"nac_first_mean_time\", \"nac_all_mean_time\",]\n",
    "    ops_x_column = [\"vertex_no\", \"monochromatic_classes_no\",]\n",
    "    ops_based_on = [\n",
    "        #  \"relabel\",\n",
    "        \"split\",\n",
    "        \"merging\",\n",
    "        ]\n",
    "    ops_aggregation = [\"mean\", \"median\", \"quartil\",]\n",
    "\n",
    "    for value_column in ops_value_column:\n",
    "        nrows = len(ops_x_column) * len(ops_based_on)\n",
    "        ncols = len(ops_aggregation)\n",
    "        fig = figure(nrows * ncols, (20, 3 * nrows), layout='constrained')\n",
    "        fig.suptitle(f\"{title} - time of NAC coloring search ({value_column})\", fontsize=20)\n",
    "        figs.append(fig)\n",
    "\n",
    "        local_df = df[df[value_column] != 0]\n",
    "        if local_df.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        row = 0\n",
    "        for x_column in ops_x_column:\n",
    "            for based_on in ops_based_on:\n",
    "                axs = [\n",
    "                    fig.add_subplot(nrows, ncols, i+ncols*row+1)\n",
    "                    for i in range(3)]\n",
    "                for ax, aggregation in zip(axs,ops_aggregation):\n",
    "                    _group_and_plot(local_df, ax, x_column, based_on, value_column, aggregation)\n",
    "                row += 1\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(fig) for fig in plot_frame(\"Laman\", df_benchmarks.query(\"dataset == 'laman'\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(fig) for fig in plot_frame(\"Laman random\", df_benchmarks.query(\"dataset == 'laman_random'\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(fig) for fig in plot_frame(\"Laman deg 3+\", df_benchmarks.query(\"dataset == 'laman_deg_3+'\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(fig) for fig in plot_frame(\"No 3 nor 4 cycles\", df_benchmarks.query(\"dataset == 'no_3_nor_4_cycles'\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(fig) for fig in plot_frame(\"Sparse\", df_benchmarks.query(\"dataset == 'sparse'\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weighted average, maybe?\n",
    "# display(plot_frame(\"General\", df_benchmarks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
