{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAC coloring search\n",
    "\n",
    "In this notebook we provide utils to run benchmarks and experiment with our code.\n",
    "\n",
    "In the first section we start with utility functions, in the second part we load/generate benchmark data. After we run individual benchmarks on selected graph classes with selected algorithms. The algorithms are described in that section.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir benchmarks/logs/nac\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from random import Random\n",
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline as backend_inline\n",
    "from matplotlib.backends import backend_agg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow as tf # unsed only for tensorboard\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import signal\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nac as nac\n",
    "from benchmarks import dataset as dataset\n",
    "\n",
    "seed=42\n",
    "TEST=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/75898999\n",
    "from typing import Callable, TypeVar, ParamSpec\n",
    "\n",
    "P = ParamSpec(\"P\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "def copy_doc(wrapper: Callable[P, T]):\n",
    "    \"\"\"An implementation of functools.wraps.\"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable[P, T]:\n",
    "        func.__doc__ = wrapper.__doc__\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@copy_doc(plt.figure)\n",
    "def figure(num: Any = 1, *args, **kwargs) -> Figure:\n",
    "    \"\"\"Creates a figure that is independent on the global plt state\"\"\"\n",
    "    fig = Figure(*args, **kwargs)\n",
    "    def show():\n",
    "        manager = backend_agg.new_figure_manager_given_figure(num, fig)\n",
    "        display(\n",
    "            manager.canvas.figure,\n",
    "            metadata=backend_inline._fetch_figure_metadata(manager.canvas.figure),\n",
    "        )\n",
    "        manager.destroy()\n",
    "    fig.show = show\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg(l: List[int]) -> int:\n",
    "    if len(l) == 0:\n",
    "        return 0\n",
    "    return sum(l) // len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeType(Enum):\n",
    "    VertexNo = \"Vertex no.\"\n",
    "    MonoClassNo = \"Monochromatic classes no.\"\n",
    "\n",
    "SIZE_TYPES = [\n",
    "    SizeType.VertexNo,\n",
    "    SizeType.MonoClassNo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarks[T](\n",
    "    function: Callable[[T, int], None],\n",
    "    params: List[T],\n",
    "    rounds: int,\n",
    "    after_benchmark: Callable[[T, int], None] = lambda x, y: None,\n",
    "    limit_sec:int|None=15,\n",
    "    seed: int|None = 42,\n",
    ") -> Dict[T, int]:\n",
    "    \"\"\"\n",
    "    Runs the given function multiple times for each parameter\n",
    "    and measures the run time.\n",
    "    Returns a dictionary mapping from the param set to mean runtime in miliseconds.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2*32)\n",
    "\n",
    "    def measure(param: T, seed: int) -> int:\n",
    "        start = time.time()\n",
    "        function(param, seed)\n",
    "        end = time.time()\n",
    "        return int((end - start) * 1000)\n",
    "\n",
    "    results: Dict[T, int] = {}\n",
    "    for param in tqdm(params):\n",
    "        try:\n",
    "            # signals are not exact, but generally work\n",
    "            if (limit_sec):\n",
    "                def timeout_handler(signum, frame):\n",
    "                    raise TimeoutError(\"Benchmark timeout\")\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(limit_sec * rounds)\n",
    "\n",
    "            rand = Random(seed)\n",
    "            mean = sum(measure(param, rand.randint(0, 2*32)) for _ in range(rounds)) // rounds\n",
    "\n",
    "            if (limit_sec):\n",
    "                signal.alarm(0)\n",
    "        except TimeoutError:\n",
    "            mean = limit_sec*1000\n",
    "\n",
    "        results[param] = mean\n",
    "        after_benchmark(param, mean)\n",
    "\n",
    "    return results\n",
    "\n",
    "if TEST:\n",
    "    display(run_benchmarks(lambda x, y: sum(range(x)), [42**4], after_benchmark=lambda x, y: print(x, y), rounds=2))\n",
    "    # display(run_benchmarks(lambda x: sum(range(x)), [42**5], rounds=2, limit_sec=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    groups: List[Tuple[str, List[int]]],\n",
    "    labels: List[str|int],\n",
    "    title: str = \"\",\n",
    "    x_label: str = \"Size\",\n",
    ") -> Figure:\n",
    "    \"\"\"\n",
    "    Takes groups of datapoints from a benchmark.\n",
    "    The items in the same group share the same color and line.\n",
    "    Each group has a name and list of datapoints\n",
    "    The second parameter gives the labels of the x axes.\n",
    "    The other parameters are selfexplanatory.\n",
    "    The function should be also able to accept pandas/numpy arrays.\n",
    "    \"\"\"\n",
    "    fig = figure()\n",
    "\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Time (ms)\")\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    for group in groups:\n",
    "        ax.plot(group[1], label=group[0])\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "if TEST:\n",
    "    display(plot_results(\n",
    "        groups = [\n",
    "            (\"Group 1\", [10, 20, 25, 28]),\n",
    "            (\"Group 2\", [15, 10, 20]),\n",
    "        ],\n",
    "        labels = ['A', 'B', 'C', 'D'],\n",
    "        title=\"Test\",\n",
    "        x_label=\"Size |V|\",\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardLogger:\n",
    "    \"\"\"\n",
    "    Stores test results into tensor board\n",
    "    \"\"\"\n",
    "\n",
    "    PATH = os.path.join(\"benchmarks\",\"logs\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            test_name: str,\n",
    "            variation: str,\n",
    "            size_type: SizeType,\n",
    "        ):\n",
    "        self.test_name = test_name\n",
    "        self.variation = variation\n",
    "        self.size_type = size_type\n",
    "\n",
    "        # Use regex and filesystem safe characters!\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        test_name = test_name.replace(\" \", \"_\")\n",
    "        match size_type:\n",
    "            case SizeType.VertexNo:\n",
    "                size_dir = \"vert\"\n",
    "            case SizeType.MonoClassNo:\n",
    "                size_dir = \"monc\"\n",
    "        self._log_dir = os.path.join(\n",
    "            TensorBoardLogger.PATH,\n",
    "            \"nac\",\n",
    "            test_name,\n",
    "            current_time,\n",
    "            size_dir,\n",
    "            variation,\n",
    "        )\n",
    "        self.writers: Dict[str, tf.summary.SummaryWriter] = {}\n",
    "\n",
    "    def _get_writter(self, group_name: str|int):\n",
    "        if group_name in self.writers:\n",
    "            return self.writers[group_name]\n",
    "        writer = tf.summary.create_file_writer(os.path.join(self._log_dir, str(group_name)))\n",
    "        self.writers[group_name] = writer\n",
    "        return writer\n",
    "\n",
    "    def log(self, group_name: str, size: int, time: int):\n",
    "        writer = self._get_writter(group_name)\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(f\"Search time ({self.test_name} {self.variation}) [{self.size_type.value}]\", time, step=size)\n",
    "        writer.flush()\n",
    "\n",
    "if TEST:\n",
    "    logger = TensorBoardLogger(\"test\")\n",
    "    for i in range(5):\n",
    "        logger.log(\"Test 1\", i, (i+1)*3)\n",
    "        logger.log(\"Test 2\", i, (i+1)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nac_benchmark_core(\n",
    "    test_name: str,\n",
    "    size_type: SizeType,\n",
    "    coloring_no_limit: int | None,\n",
    "    rounds: int,\n",
    "    relabel_strategies: List[str],\n",
    "    split_strategies: List[str],\n",
    "    merge_strategies: List[str],\n",
    "    subgraph_sizes: List[int],\n",
    "    dataset: Dict[int, List[nx.Graph]],\n",
    "    verbose: bool = True,\n",
    "    seed: int | None = 42,\n",
    ") -> List[Dict[str, Dict[int, List[int]]]]:\n",
    "    \"\"\"\n",
    "    Runs benchmarks for NAC coloring search\n",
    "    Returns results grouped by relabel, split, merge and subgraph size strategies\n",
    "    \"\"\"\n",
    "    if coloring_no_limit is None:\n",
    "        coloring_no_limit = 2**30\n",
    "\n",
    "    baseLogger = TensorBoardLogger(test_name, \"combined\", size_type)\n",
    "    loggers = [\n",
    "        TensorBoardLogger(test_name, \"relabel\", size_type),\n",
    "        TensorBoardLogger(test_name, \"split\", size_type),\n",
    "        TensorBoardLogger(test_name, \"merge\", size_type),\n",
    "        TensorBoardLogger(test_name, \"subgraph\", size_type),\n",
    "    ]\n",
    "\n",
    "    # for each strategy holds a dictionary mapping from graph size to list of results\n",
    "    results: List[Dict[str, Dict[int, List[int]]]] = [\n",
    "        defaultdict(lambda: defaultdict(list)) for _ in range(4)\n",
    "    ]\n",
    "\n",
    "    for graph_size, graphs in dataset.items():\n",
    "        if verbose:\n",
    "            print(f\"Starting with a new graphs size {graph_size} ({len(graphs)})\")\n",
    "\n",
    "        def algo_name(param: Tuple[str, str, str, int]) -> str:\n",
    "            _, split, merge, subgraph = param\n",
    "            return \"subgraphs-{}-{}-{}-smart\".format(\n",
    "                merge, split, subgraph\n",
    "            )\n",
    "\n",
    "        def find_colorings(param: Tuple[str, str, str, int], seed: int):\n",
    "            relabel, split, merge, subgraph = param\n",
    "            for graph in graphs:\n",
    "                # print(f\"Search start: {relabel}-{algo_name(param)}\")\n",
    "                for coloring, _ in zip(\n",
    "                    nac.NAC_colorings(\n",
    "                        graph=graph,\n",
    "                        algorithm=algo_name(param),\n",
    "                        relabel_strategy=relabel,\n",
    "                        seed=seed,\n",
    "                    ),\n",
    "                    range(coloring_no_limit),\n",
    "                ): pass\n",
    "\n",
    "        def callback(param: Tuple[str, str, str, int], time: int):\n",
    "            baseLogger.log(param[0] + '_' + algo_name(param), graph_size, time)\n",
    "\n",
    "        params = list(itertools.product(\n",
    "            relabel_strategies, split_strategies, merge_strategies, subgraph_sizes\n",
    "        ))\n",
    "        param_to_time = run_benchmarks(\n",
    "            function=find_colorings,\n",
    "            params=params,\n",
    "            rounds=rounds,\n",
    "            after_benchmark=callback,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        times_dicts = [defaultdict(list) for _ in range(4)]\n",
    "        for param, time in param_to_time.items():\n",
    "            for name, dest, times_dict in zip(param, results, times_dicts):\n",
    "                dest[name][graph_size].append(time)\n",
    "                times_dict[name].append(time)\n",
    "        for times_dic, logger in zip(times_dicts, loggers):\n",
    "            for name, times in times_dic.items():\n",
    "                logger.log(name, graph_size, avg(times))\n",
    "\n",
    "    return results\n",
    "\n",
    "if TEST:\n",
    "    test_bench_result = nac_benchmark_core(\n",
    "        test_name=\"Test run\",\n",
    "        size_type=SizeType.VertexNo,\n",
    "        coloring_no_limit=None,\n",
    "        rounds=2,\n",
    "        relabel_strategies=[\"random\"],\n",
    "        split_strategies= [\"none\", \"neighbors\"],\n",
    "        merge_strategies= [\"linear\"],\n",
    "        subgraph_sizes = [4],\n",
    "        dataset= {\n",
    "            20: [nx.Graph([(1, 2), (2, 3)])],\n",
    "            22: [nx.Graph([(1, 2), (2, 3), (3, 4)])],\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_for_plotting(data: Dict[str, Dict[int, List[int]]]) -> Tuple[List[Tuple[str, List[int]]], List[str|int]]:\n",
    "    res: List[Tuple[str, List[int]]] = []\n",
    "    sizes: List[str|int] = []\n",
    "    for name, measurements_for_sizes in data.items():\n",
    "        if len(measurements_for_sizes) > len(sizes):\n",
    "            sizes = [size for size, _ in measurements_for_sizes.items()]\n",
    "\n",
    "        res.append(\n",
    "            (\n",
    "                name,\n",
    "                [y for _, y in sorted(\n",
    "                    (size, avg(measurement)) for size, measurement in measurements_for_sizes.items()\n",
    "                )],\n",
    "            )\n",
    "        )\n",
    "    return res, sizes\n",
    "\n",
    "def plot_benchmark_results(\n",
    "    bench_result: List[Dict[str, Dict[int, List[int]]]],\n",
    "    size_type: SizeType,\n",
    "):\n",
    "    for name, cathegory in zip([\"Relabel\", \"Split\", \"Merge\", \"Subgraph_size\"], bench_result):\n",
    "        transformed, labels = _transform_for_plotting(cathegory)\n",
    "        if len(transformed) <= 1:\n",
    "            continue\n",
    "        display(plot_results(transformed, labels=labels, title=f\"{name} ({size_type.value})\", x_label=size_type.value))\n",
    "\n",
    "if TEST:\n",
    "    plot_benchmark_results(test_bench_result, SizeType.VertexNo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_vertex_no[T: nx.Graph](graphs: Iterable[T]) -> Dict[int, List[T]]:\n",
    "    res: defaultdict[int, List[T]] = defaultdict(list)\n",
    "    for graph in graphs:\n",
    "        res[nx.number_of_nodes(graph)].append(graph)\n",
    "    return res\n",
    "\n",
    "\n",
    "def group_by_monochomatic_classes_no[T: nx.Graph ](graphs: Iterable[T],) -> Dict[int, List[T]]:\n",
    "    res: defaultdict[int, List[T]] = defaultdict(list)\n",
    "    for graph in graphs:\n",
    "        classes = len(nac.find_triangle_components(graph)[1])\n",
    "        res[classes].append(graph)\n",
    "    return res\n",
    "\n",
    "def group_dataset[T: nx.Graph](dataset: List[T]) -> Tuple[Dict[int, List[T]], Dict[int, List[T]]]:\n",
    "    return group_by_vertex_no(dataset), group_by_monochomatic_classes_no(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphs:\n",
    "    laman = group_dataset(list(dataset.load_laman_graphs()))\n",
    "    laman_deg_3_plus = group_dataset(list(dataset.load_laman_degree_3_plus()))\n",
    "    no_3_nor_4_cycles = group_dataset(dataset.load_no_3_nor_4_cycle_graphs())\n",
    "    sparse_graphs = group_dataset(dataset.generate_sparse_graphs(30, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_datasets():\n",
    "    def sumarize(title: str, classes: Tuple[Dict[int, List[nx.Graph]], Dict[int, List[nx.Graph]]]):\n",
    "        print(title)\n",
    "        print(sorted([(y, len(x)) for y, x in classes[0].items()]),)\n",
    "        print(sorted([(y, len(x)) for y, x in classes[1].items()]),)\n",
    "        print()\n",
    "\n",
    "    sumarize(\n",
    "        \"Laman\",\n",
    "        Graphs.laman,\n",
    "    )\n",
    "    sumarize(\n",
    "        \"Laman deg 3+\",\n",
    "        Graphs.laman_deg_3_plus,\n",
    "    )\n",
    "    sumarize(\n",
    "        \"No 3 nor 4 cycles\",\n",
    "        Graphs.no_3_nor_4_cycles,\n",
    "    )\n",
    "    sumarize(\n",
    "        \"Sparse\",\n",
    "        Graphs.sparse_graphs,\n",
    "    )\n",
    "print_graph_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(groups: Dict[int, List[nx.Graph]], group_id: int) -> Dict[int, List[nx.Graph]]:\n",
    "    return {group_id: groups[group_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks, Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Promissing:\n",
    "    RELABELING = [\n",
    "        \"none\",\n",
    "        \"random\",\n",
    "        \"bfs\",\n",
    "    ]\n",
    "    SPLITTING = [\n",
    "        \"none\",\n",
    "        \"neighbors\",\n",
    "        \"neighbors_degree\",\n",
    "    ]\n",
    "    MERGING_OFFLINE = [\n",
    "        \"linear\",\n",
    "        \"log\",\n",
    "        \"score\",\n",
    "        \"shared_vertices\"\n",
    "    ]\n",
    "    MERGING_ONLINE = [\n",
    "        \"linear\",\n",
    "        \"log\",\n",
    "        \"shared_vertices\"\n",
    "    ]\n",
    "    SIZES = [6]\n",
    "\n",
    "def run_promissing_core(\n",
    "    test_name: str,\n",
    "    rounds: int,\n",
    "    coloring_no_limit: int|None,\n",
    "    group_limits: Tuple[List[Tuple[int, int, int]], List[Tuple[int, int, int]]],\n",
    "    dataset: Tuple[Dict[int, List[nx.Graph]], Dict[int, List[nx.Graph]]],\n",
    "    relabel_strategies:List[str],\n",
    "    split_strategies:List[str],\n",
    "    merge_strategies:List[str],\n",
    "    subgraph_sizes:List[int],\n",
    "    verbose: bool,\n",
    "):\n",
    "    res = []\n",
    "    for limit, right_dataset, size_type in zip(group_limits, dataset, SIZE_TYPES):\n",
    "        if verbose:\n",
    "            print(f\"Starting search ({size_type.value})\")\n",
    "        updated_dataset: Dict[int, List[nx.Graph]] = defaultdict(list)\n",
    "        for size_start, size_end, graph_no in limit:\n",
    "            for size in range(size_start, size_end+1):\n",
    "                current = right_dataset[size]\n",
    "                if len(current) == 0 or len(current) < graph_no:\n",
    "                    continue\n",
    "                updated_dataset[size] = current[:graph_no]\n",
    "\n",
    "        results = nac_benchmark_core(\n",
    "            test_name=test_name,\n",
    "            size_type=size_type,\n",
    "            coloring_no_limit=coloring_no_limit,\n",
    "            relabel_strategies=relabel_strategies,\n",
    "            split_strategies=split_strategies,\n",
    "            merge_strategies=merge_strategies,\n",
    "            subgraph_sizes=subgraph_sizes,\n",
    "            dataset=updated_dataset,\n",
    "            rounds=rounds,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        res.append(results)\n",
    "    for results, type in zip(res, SIZE_TYPES):\n",
    "        plot_benchmark_results(results, type)\n",
    "\n",
    "    return tuple(res)\n",
    "\n",
    "def run_promissing_all(\n",
    "    test_name: str,\n",
    "    rounds: int,\n",
    "    group_limits: Tuple[List[Tuple[int, int, int]], List[Tuple[int, int, int]]],\n",
    "    dataset: Tuple[Dict[int, List[nx.Graph]], Dict[int, List[nx.Graph]]],\n",
    "    relabel_strategies:List[str]=Promissing.RELABELING,\n",
    "    split_strategies:List[str]=Promissing.SPLITTING,\n",
    "    merge_strategies:List[str]=Promissing.MERGING_OFFLINE,\n",
    "    subgraph_sizes:List[int]=Promissing.SIZES,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    return run_promissing_core(\n",
    "        test_name=test_name,\n",
    "        rounds=rounds,\n",
    "        coloring_no_limit=None,\n",
    "        group_limits=group_limits,\n",
    "        dataset=dataset,\n",
    "        relabel_strategies=relabel_strategies,\n",
    "        split_strategies=split_strategies,\n",
    "        merge_strategies=merge_strategies,\n",
    "        subgraph_sizes=subgraph_sizes,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def run_promissing_single(\n",
    "    test_name: str,\n",
    "    rounds: int,\n",
    "    group_limits: Tuple[List[Tuple[int, int, int]], List[Tuple[int, int, int]]],\n",
    "    dataset: Tuple[Dict[int, List[nx.Graph]], Dict[int, List[nx.Graph]]],\n",
    "    relabel_strategies:List[str]=Promissing.RELABELING,\n",
    "    split_strategies:List[str]=Promissing.SPLITTING,\n",
    "    merge_strategies:List[str]=Promissing.MERGING_ONLINE,\n",
    "    subgraph_sizes:List[int]=Promissing.SIZES,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    return run_promissing_core(\n",
    "        test_name=test_name,\n",
    "        rounds=rounds,\n",
    "        coloring_no_limit=1,\n",
    "        group_limits=group_limits,\n",
    "        dataset=dataset,\n",
    "        relabel_strategies=relabel_strategies,\n",
    "        split_strategies=split_strategies,\n",
    "        merge_strategies=merge_strategies,\n",
    "        subgraph_sizes=subgraph_sizes,\n",
    "        verbose=verbose,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laman all colorings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_search_res = run_promissing_all(\n",
    "        test_name=\"Test\",\n",
    "        rounds=1,\n",
    "        group_limits=(\n",
    "            [(10, 13, 8)],\n",
    "            [(8, 11, 8)],\n",
    "        ),\n",
    "        dataset=Graphs.laman,\n",
    "    )\n",
    "if TEST:\n",
    "    test_search_res = run_promissing_single(\n",
    "        test_name=\"Test\",\n",
    "        rounds=3,\n",
    "        group_limits=(\n",
    "            [(10, 13, 8)],\n",
    "            [(8, 11, 8)],\n",
    "        ),\n",
    "        dataset=Graphs.laman,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_res = run_promissing_all(\n",
    "    test_name=\"Laman all\",\n",
    "    rounds=2,\n",
    "    group_limits=(\n",
    "        [(12, 17, 20)],\n",
    "        [(10, 14, 20)],\n",
    "    ),\n",
    "    dataset=Graphs.laman,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_res2 = run_promissing_all(\n",
    "    test_name=\"Laman all\",\n",
    "    rounds=2,\n",
    "    group_limits=(\n",
    "        [(17, 20, 5)],\n",
    "        [(14, 18, 5)],\n",
    "    ),\n",
    "    dataset=Graphs.laman,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_res = run_promissing_single(\n",
    "    test_name=\"Laman single\",\n",
    "    rounds=3,\n",
    "    group_limits=(\n",
    "        [(28, 30, 24)],\n",
    "        [(50, 60, 11)],\n",
    "    ),\n",
    "    dataset=Graphs.laman,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
